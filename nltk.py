from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer


input_text = """Do you know how tokenization
            works? It's actually quite interesting!
            Let's analyse a couple of sentences and figure it out."""

print("\n sentence tokenizer:")
print(word_tokenize(input_text))




